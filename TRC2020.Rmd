---
title: "Ensemble feature selection"
author: "Dmitry Pavlyuk"
date: "March 24, 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

This markdown document reproduces the research "Robust learning of urban traffic flows' spatiotemporal structure" (to be submitted)


```{r child='sampling.Rmd'}
```


Load necessary libraries
----------------
```{r libs}
library(needs)
needs(knitr)
needs(tidyverse)
needs(reshape2)
needs(ggplot2)
needs(geosphere)
needs(igraph)
needs(Metrics)
needs(imputeTS)
needs(e1071)
needs(MTS)
needs(forecast)
needs(doParallel)
needs(glasso)
needs(scales)
needs(stringr)
needs(e1071 )
needs(randomForest )
needs(Boruta)
needs(matrixStats)
needs(fsMTS)
needs(plot.matrix)
needs(lubridate)

source(file.path("R","prepare_data_functions.R"))
source(file.path("R","cv_functions.R"))
source(file.path("R","cv_utils.R"))
source(file.path("R","models_util.R"))
source(file.path("R","models_baseline.R"))
source(file.path("R","models_spvar.R"))


# Define files for intermediate results
dir<-"TRC2020"
models.estimated.file <- file.path(dir,"models.rds")
results.file <- file.path(dir,"results.rds")
complete.rds <- file.path("data","prepared","ALL5-original.rds")
sample.rds <- file.path(dir,"data64.rds")
ta<-5
```



```{r new_sampling}

if (!file.exists(sample.rds)){
mysample <- readRDS(complete.rds)
colnames(mysample$data)<-gsub('.volume','',colnames(mysample$data))
series<-colnames(mysample$data)[-1]
central.node<-"rnd_90797"
center<-mysample$config.nodes%>%filter(node_name==central.node)%>%select(node_lon,node_lat)%>%as.list
nodes<-mysample$config.nodes%>%filter(node_name %in% series)%>%rowwise()%>%filter(distHaversine(c(node_lon,node_lat),c(center$node_lon,center$node_lat))<25000)
write.csv(nodes,file.path(dir,"nodes-all.csv"), row.names = F)

series <- nodes%>%select(node_name)%>%pull

morning.rush<-mysample$data%>%filter(hour(datetime)>=7 & hour(datetime)<10 & !(weekdays(datetime)%in%c("Sunday", "Saturday")))

evening.rush<-mysample$data%>%filter(hour(datetime)>=17 & hour(datetime)<20 & !(weekdays(datetime)%in%c("Sunday", "Saturday")))

selectMostImportant <- function(data, series, rho, topn, shortest, distance.lim=4){
  m.cor<-stats::cor(data[,series])
  m.cor[is.na(m.cor)]<-0
  gl.m <- glasso::glasso(m.cor,rho=rho)
  rs.m<-as_tibble(rowSums(abs(gl.m$wi)>0))%>%mutate(node=series)
  hist(rs.m%>%select(value)%>%pull)
  s.m<-rs.m%>%top_n(topn, wt=value)%>%arrange(desc(value))%>%select(node)%>%pull
  s.m.filtered <- c()
  for (s in s.m){
    ser<-c(s,s.m.filtered)
    m<-shortest[ser,ser]
    if (min(m+(distance.lim+0.01)*diag(ncol(m)))>=distance.lim){
      s.m.filtered<-append(s.m.filtered,s)
    }
  }
  return(s.m.filtered)
}


morning.nodes <- selectMostImportant(morning.rush, series = c(series), 
                    rho=0.3, topn = 50, shortest=mysample$shortest.distances,distance.lim=1)
length(morning.nodes)
write.csv(mysample$config.nodes%>%filter(node_name %in% morning.nodes),file.path(dir,"nodes-MR50.csv"),  row.names = F)

evening.nodes <- selectMostImportant(evening.rush, series = c(series), 
                    rho=0.3, topn = 50, shortest=mysample$shortest.distances,distance.lim=1)
length(evening.nodes)
write.csv(mysample$config.nodes%>%filter(node_name %in% evening.nodes),file.path(dir,"nodes-ER50.csv"),  row.names = F)

write.csv(mysample$config.nodes%>%filter(node_name %in% gsub('.volume','',intersect(morning.nodes,evening.nodes))),file.path(dir,"nodes-ER+MR.csv"),  row.names = F)
intersect(morning.nodes,evening.nodes)
length(union(morning.nodes,evening.nodes))
series <- union(morning.nodes,evening.nodes)

dat<-mysample$data.orig%>%gather(key = "node", value="value", -datetime)%>%filter(node %in% paste0(series,".volume"))%>%mutate(node=gsub(".volume","",node), dt=as.POSIXct(format(datetime, "%H:%M"),format="%H:%M",tz="GMT"))%>%group_by(node,dt)%>%summarize(value=median(value))
date.labels = seq(from = min(dat$dt),
                 to = max(dat$dt),
                 by = "hour")
dat%>%filter(node %in% morning.nodes)%>%ggplot(aes(x = dt, y = value, col=node, group=node)) + geom_line()+ 
  theme(legend.position = "none")+scale_x_datetime(labels = date_format("%H:%M"),
                     date_breaks = "4 hours")+ xlab("TIme of the day") + ylab("Traffic volume")
dat%>%filter(node %in% evening.nodes)%>%ggplot(aes(x = dt, y = value, col=node, group=node))  + geom_line()+ 
  theme(legend.position = "none")+scale_x_datetime(labels = date_format("%H:%M"),
                     date_breaks = "4 hours")+ xlab("TIme of the day") + ylab("Traffic volume")

sampl<-list(data = mysample$data[,c("datetime",series)],
               shortest.distances=mysample$shortest.distances[series,series])
saveRDS(sampl, file=sample.rds)
}else{
  sampl <- readRDS(sample.rds)
}
print(ncol(sampl$data))
series <- colnames(sampl$data)[-1]

# Prepare a matrix of maximal lags (for travel time regularisation)
lagMatrix <- round(sampl$shortest.distances / ta)
lagMatrix[is.infinite(lagMatrix)]<-0
lagMatrix<-lagMatrix[as.vector(series),as.vector(series)]


shortestA <- sampl$shortest.distances

d<-c()
for (i in seq(1, 60)){
  d<-append(d,length(shortestA[shortestA<i])/ncol(shortestA))
}
dat<-tibble(x=seq(1, 60), d=d)
ggplot(dat, aes(x, d)) + geom_line(colour="red")+xlab("Travel time")+ylab("% of reached nodes")

d<-tibble(av=as.vector(shortestA[shortestA<Inf]))
ggplot(d, aes(av)) + geom_density(alpha = 0.1,fill="red", colour="red")

gr<-shortestA
gr[gr>15]<-0
mean(gr[gr>0])
needs(igraph)
g<-graph_from_adjacency_matrix(gr, mode = "directed", weighted = T)
#tkplot(g, vertex.size=7, vertex.color="lightblue", vertex.label=NA,edge.arrow.size=0.5, edge.width=E(g)$weight*0.1)
comp<-components(g, mode = "weak")
```


```{r fs}

source(file.path("R","cv_functions.R"))
data.scaled<-cbind(sampl$data[,1],scale(sampl$data[,-1]))
estimateFeatures(fs.function=fsMTS::fsMTS, 
                 data = sampl$data, 
                 seriesNames = series,
                 trainingWindowSize = 4032,#nrow(sampl$data),#12*24*0.5,
                 forecastEvery = 1,
                 max.lag = 3,
                 fs.folder = file.path(dir,"fs",length(series),"MIloc"),
                 req.packages = c("fsMTS"),
                 method="MI",
                 rho=0.3,
                 shortest = shortestA,
                 step = ta,
                 localized = T,
                 show.progress=T,
                 clusterNumber=1
                 )


source(file.path("R","cv_functions.R"))
estimateFeaturesGlobal(fs.function=fsMTS::fsMTS, 
                 data = sampl$data, 
                 seriesNames = series,
                 trainingWindowSize = 36,
                 forecastEvery = 1,
                 max.lag = 3,
                 ta=ta,
                 fs.folder = file.path(dir,"fs",length(series),"MIg"),
                 req.packages = c("fsMTS","lubridate","tibble","tidyverse"),
                 method="MI",
                 rho=0.3,
                 shortest = shortestA,
                 req.functions=c("daySec"),
                 step = ta,
                 localized = F,
                 clusterNumber = 1
                 )


from<-daySec(as.POSIXct("00:05", format="%H:%M"))
wd <- wday(as.POSIXct("00:25", format="%H:%M")-from)
rto<-daySec(as.POSIXct("00:25", format="%H:%M"))
to<-rto + 3*ta*60
dif<-ifelse(to>from,to-from,to+3600*24-from)
dif2<-ifelse(rto>from,rto-from,rto+3600*24-from)

sampl$data[5,]$datetime
sampl$data[5,]$datetime - from

dat<-sampl$data%>%mutate(h=daySec(datetime - from))%>%filter(h<=dif,wday(datetime-from)==wd)%>%arrange(datetime)
dat[dat$h>dif2,-1]<-NA
dat
fsMTS::fsMTS(as.matrix(dat%>%select(-one_of("datetime","h"))),max.lag = 3, method = "CCF")

```

```{r models}
RunModels <- function(data, series, ta, forecastingSteps, forecastEvery,
                      validationSize, validationEnd, 
                      lagMatrix,shortestA, cvgrid, results.folder){
  dir.create(results.folder, recursive = T)
  start.time <- Sys.time()
  results<-tibble()
  models.estimated <- c()
  if (file.exists(models.estimated.file)) models.estimated <- readRDS(models.estimated.file)
  else saveRDS(models.estimated, models.estimated.file)
  if (file.exists(results.file)) results <- readRDS(results.file)
  else saveRDS(results, results.file)
  for (r in 1:nrow(cvgrid)){
    cv <- cvgrid[r,]
    print(paste("Tuned parameter's set",r,"of",nrow(cvgrid)))
    print(cv)
    validationStart <- validationEnd - validationSize
    trainingSize <- cv$trainingMinutes/ta
    dat.restricted<-data[(validationStart-trainingSize+1):validationEnd,]
    base_params<-list(data=dat.restricted, seriesNames=series,
                      forecastingSteps=forecastingSteps, forecastEvery=forecastEvery)
    params<-c(base_params,list(trainingWindowSize=trainingSize))
    
    if (length(forecastingSteps)>1) params$forecastingSteps=max(forecastingSteps)
    #estimate.HA(params,cv,results.file, models.estimated.file)
    #estimate.naive(params,cv,results.file, models.estimated.file)
    #estimate.arima(params,cv,results.file, models.estimated.file)
    #estimate.SpVAR(params,cv,results.folder)
    estimate.KNN(params,cv,results.folder)
    print(Sys.time() - start.time)
  }
}
```

```{r experiments}
dat<-as.data.frame(sampl$data)
rownames(dat)<-format(dat$datetime,"%Y-%m-%d %H:%M:%S")

# source(file.path("R","models_spvar.R"))
# xModel.star$run(dat[1:1440,series], forecastingSteps=3, arLags=3,
#                 fs.folder=RFg.folder,
#                 control=list(),
#                 returnModel=F,
#                 refine = F,
#                 include.mean=F,
#                 verbose=T,
#                 exclude.series=NULL,
#                 threshold=0.1)


week<-24*60*7
data<-sampl$data[-seq(1:(7*12)),]
validationSize <- nrow(data)-3*week/ta
validationEnd <- nrow(data)
forecastEvery<-12*24
forecastingSteps<-12


maxlag<-3
CCF.folder <- file.path(dir,"fs",length(series),"CCF",36,maxlag)
MI.folder <- file.path(dir,"fs",length(series),"MI",36,maxlag)
GLASSO.folder <- file.path(dir,"fs",length(series),"GLASSOloc",36,maxlag,0.3)
LARS.folder <- file.path(dir,"fs",length(series),"LARS",36,maxlag)
RF.folder <- file.path(dir,"fs",length(series),"RF",36,maxlag)

CCFg.folder <- file.path(dir,"fs",length(series),"CCFg",36,maxlag)
MIg.folder <- file.path(dir,"fs",length(series),"MIg",36,maxlag)
GLASSOg.folder <- file.path(dir,"fs",length(series),"GLASSOlocg",36,maxlag,0.3)
LARSg.folder <- file.path(dir,"fs",length(series),"LARSg",36,maxlag)
RFg.folder <- file.path(dir,"fs",length(series),"RFg",36,maxlag)
EnsVotingg.folder <- file.path(dir,"fs",length(series),"EnsVotingg",36,maxlag,0.2)
EnsRankingg.folder <- file.path(dir,"fs",length(series),"EnsRankingg",36,maxlag,0.2)

distance.folder <- file.path(dir,"fs",length(series),"distance",20160,maxlag,"2017-10-08_000000.rds")
CCF.global <- file.path(dir,"fs",length(series),"CCF",20160,maxlag,"2017-10-08_000000.rds")
GLASSO.global <- file.path(dir,"fs",length(series),"GLASSOloc",20160,maxlag,0.3,"2017-10-08_000000.rds")
MI.global <- file.path(dir,"fs",length(series),"MI",4032,maxlag,"2017-08-13_000000.rds")
LARS.global <- file.path(dir,"fs",length(series),"LARS",20160,maxlag,"2017-10-08_000000.rds")
RF.global <- file.path(dir,"fs",length(series),"RF",20160,maxlag,"2017-10-08_000000.rds")
EnsVoting.global <- file.path(dir,"fs",length(series),"EnsVoting",20160,maxlag,0.2,"2017-10-08_000000.rds")
EnsRanking.global <- file.path(dir,"fs",length(series),"EnsRanking",20160,maxlag,0.2,"2017-10-08_000000.rds")


# Define a grid for tuning parameters
cvgrid <- expand.grid(trainingMinutes = c(week),
                      include.mean = c(F),
                      arLags = c(maxlag),
                      stationary =  c(T),
                      allowdrift =  c(F),
                      fs.folder=c(NA,CCF.global,GLASSO.global,MI.global,LARS.global,RF.global),
                      threshold=c(0.01,0.05,seq(0.1,0.9,by=0.1)),
                      ownlags=c(T),
                      kneighbours=c(36))
source(file.path("R","models_spvar.R"))
source(file.path("R","cv_functions.R"))
# Run all models
RunModels(data, series, ta, forecastingSteps, forecastEvery,
                      validationSize, validationEnd, 
                      lagMatrix, shortestA, cvgrid,
                      results.folder="thtrain")
 
```

```{r knn}
needs(FNN)


mts<-dat[1:10,series[1:3]]

max.lag<-3

source(file.path("R","cv_functions.R"))
source(file.path("R","cv_utils.R"))
source(file.path("R","models_spvar.R"))

(knnForecast(dat[1:10080,series],12,3,0.1,fs.folder=EnsVoting.global,ownlags=F,control=list(kneighbours=36)))

```


```{r analyse_results}
#params <- c('trainingMinutes','allowdrift','stationary','arLags','include.mean','fs.folder','threshold','ownlags')

# models.estimated <- readRDS(file.path(dir,"baseline.models.rds"))
# 
# results <- readRDS(results.file)
# models.estimated <- readRDS(models.estimated.file)
# 
# results<-results%>%mutate(fs.folder=gsub(file.path(dir,"fs",64),"",fs.folder))
# summary(results$actual)
# #results<-results%>%filter(abs(actual)>100)
# #results%>%select(datetime,detector,actual,forecasted)
# actu<-mysample$data.orig%>%gather(key="detector", value="actual.orig", -one_of("datetime"))%>%
#   mutate(detector=gsub(".volume","",detector))
# r<-results%>%left_join(actu, by=c("datetime"="datetime","detector"="detector"))
# r1<-r%>%mutate(actual=actual+actual.orig,forecasted=forecasted+actual.orig)
# 
# r1<-results

reslist<-list()
fold<-"thtrain"
for (f in list.files(fold)){
  reslist[[length(reslist)+1]]<-readRDS(file.path(fold, f))
}
results<-bind_rows(reslist)

options(pillar.sigfig = 4)
#results <- readRDS("KNN_TRC2020_fs_64_CCF_20160_3_2017-10-08_000000_rds_10080_3_36_0_1_TRUE.rds")
#r1<-results%>%filter(daySec(datetime)>7*60*60,daySec(datetime)<10*60*60)
# params <- c('trainingMinutes','arLags','include.mean','fs.folder','threshold','ownlags')
# s<-cvSummary(results, mae, cumulative=F, params=params)%>%
#     mutate(modelid=paste(model,trainingMinutes, arLags, include.mean,fs.folder,threshold,ownlags))%>%
#     mutate(modelid=gsub(" NA","",modelid))
params <- c('trainingMinutes','arLags','fs.folder','threshold','ownlags','kneighbours')
s<-cvSummary(results, mae, cumulative=F, params=params)%>%
    mutate(modelid=paste(model,trainingMinutes, arLags,fs.folder,threshold,ownlags,kneighbours))%>%
    mutate(modelid=gsub(" NA","",modelid))


s%>%filter(forecast_horizon<13)%>%ungroup%>%mutate(key=paste(model,trainingMinutes,fs.folder))%>%select(key,threshold,forecast_horizon,MAE)%>%filter(forecast_horizon==3)%>%
  spread(key=threshold, value=MAE)

#saveRDS(s, summary.file)

s%>%
    group_by(modelid,forecast_horizon)%>%filter(MAE==min(MAE))%>%ungroup%>%select(-c('allowdrift','stationary','include.mean'))%>%print(n=50)

pres<-results%>%filter(forecast_horizon==12)%>%mutate(last_date_d = format(datetime, format="%w %H:%M",tz="GMT"))%>%group_by(model,last_date_d)%>%summarise(mean.mae=mae(actual, forecasted))%>%arrange(last_date_d)
pres%>%ggplot(aes(x = last_date_d, y=mean.mae, group=1)) +geom_line()

(winners3 <- s%>%
    filter(forecast_horizon==3)%>%
    group_by(model)%>%filter(MAE==min(MAE))%>%ungroup%>%select(-c('allowdrift','stationary','include.mean')))


 results.cum <-results%>%
    mutate(modelid=gsub(" NA","",paste(model,trainingMinutes, arLags, ccfThreshold,
                         include.mean,stationary,allowdrift,radius1,radius2,include_ratio, spatial_step,modelsp,decay)))%>%
    group_by(model_hash,detector)%>%
    arrange(model_hash,detector,forecast_horizon)%>%
    mutate(cumactual=cumsum(actual),cumforecasted=cumsum(forecasted))

#tmodels<-c("autoarima 7200 FALSE FALSE FALSE","SpatialARIMAX 7200 FALSE FALSE FALSE 10 30 FALSE 5",           "SpatialSVR 7200 10 30 FALSE 5 svm","STAR ccf 7200 6 0.1 FALSE")
 tmodels<-c("autoarima 7200 FALSE FALSE FALSE","SpatialARIMAX 7200 FALSE FALSE FALSE 10 30 FALSE 5",           "SpatialSVR 7200 10 30 FALSE 5 svm")
results.cum.filtered<-results.cum%>%filter(forecast_horizon==3,modelid %in% tmodels)%>% mutate(modelid=ifelse(str_detect(modelid,"^STAR ccf"),"SpVAR-cc",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^SpatialARIMAX"),"SpX-ARIMAX",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^SpatialSVR"),"SpX-SVR",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^autoarima"),"ARIMA",modelid))
 
gtmp1<-results.cum.filtered%>%mutate(last_date_d = format(datetime, format="%H:%M",tz="GMT"))

gtmp2<-gtmp1%>%group_by(modelid,last_date_d)%>%summarise(mn=mae(cumactual,cumforecasted))
gtmp2%>%ggplot(aes(x = mn, group=modelid, fill=modelid)) +
  geom_density(kernel = "epanechnikov", alpha = 0.4)+
  scale_x_continuous(expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+labs(x="Temporarily averaged MAE", fill="Model")
 
gtmp2<-gtmp1%>%group_by(modelid,detector)%>%summarise(mn=mae(cumactual,cumforecasted))
gtmp2%>%ggplot(aes(x = mn, group=modelid, fill=modelid)) +
  geom_density(kernel = "epanechnikov", alpha = 0.4)+
  scale_x_continuous(expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+labs(x="Spatially averaged MAE", fill="Model")
 
```