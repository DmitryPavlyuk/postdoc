---
title: "Ensemble feature selection"
author: "Dmitry Pavlyuk"
date: "March 24, 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

This markdown document reproduces the research "Robust learning of urban traffic flows' spatiotemporal structure" (to be submitted)


```{r child='sampling.Rmd'}
```


Load necessary libraries
----------------
```{r libs}
library(needs)
needs(knitr)
needs(tidyverse)
needs(reshape2)
needs(ggplot2)
needs(geosphere)
needs(igraph)
needs(Metrics)
needs(imputeTS)
needs(e1071)
needs(MTS)
needs(forecast)
needs(doParallel)
needs(glasso)
needs(scales)
needs(stringr)
needs(e1071 )
needs(randomForest )
needs(Boruta)

source(file.path("R","prepare_data_functions.R"))
source(file.path("R","cv_functions.R"))
source(file.path("R","cv_utils.R"))
source(file.path("R","models_util.R"))
source(file.path("R","models_baseline.R"))
source(file.path("R","models_spvar.R"))


# Define files for intermediate results
dir<-"TRC2020"
models.estimated.file <- file.path(dir,"models.rds")
results.file <- file.path(dir,"results.rds")
complete.rds <- file.path("data","prepared","ALL5-original.rds")
sample.rds <- file.path(dir,"data64.rds")
ta<-5
```



```{r new_sampling}

if (!file.exists(sample.rds)){
mysample <- readRDS(complete.rds)
colnames(mysample$data)<-gsub('.volume','',colnames(mysample$data))
series<-colnames(mysample$data)[-1]
central.node<-"rnd_90797"
center<-mysample$config.nodes%>%filter(node_name==central.node)%>%select(node_lon,node_lat)%>%as.list
nodes<-mysample$config.nodes%>%filter(node_name %in% series)%>%rowwise()%>%filter(distHaversine(c(node_lon,node_lat),c(center$node_lon,center$node_lat))<25000)
write.csv(nodes,file.path(dir,"nodes-all.csv"), row.names = F)

series <- nodes%>%select(node_name)%>%pull

morning.rush<-mysample$data%>%filter(hour(datetime)>=7 & hour(datetime)<10 & !(weekdays(datetime)%in%c("Sunday", "Saturday")))

evening.rush<-mysample$data%>%filter(hour(datetime)>=17 & hour(datetime)<20 & !(weekdays(datetime)%in%c("Sunday", "Saturday")))

selectMostImportant <- function(data, series, rho, topn, shortest, distance.lim=4){
  m.cor<-stats::cor(data[,series])
  m.cor[is.na(m.cor)]<-0
  gl.m <- glasso::glasso(m.cor,rho=rho)
  rs.m<-as_tibble(rowSums(abs(gl.m$wi)>0))%>%mutate(node=series)
  hist(rs.m%>%select(value)%>%pull)
  s.m<-rs.m%>%top_n(topn, wt=value)%>%arrange(desc(value))%>%select(node)%>%pull
  s.m.filtered <- c()
  for (s in s.m){
    ser<-c(s,s.m.filtered)
    m<-shortest[ser,ser]
    if (min(m+(distance.lim+0.01)*diag(ncol(m)))>=distance.lim){
      s.m.filtered<-append(s.m.filtered,s)
    }
  }
  return(s.m.filtered)
}


morning.nodes <- selectMostImportant(morning.rush, series = c(series), 
                    rho=0.3, topn = 50, shortest=mysample$shortest.distances,distance.lim=1)
length(morning.nodes)
write.csv(mysample$config.nodes%>%filter(node_name %in% morning.nodes),file.path(dir,"nodes-MR50.csv"),  row.names = F)

evening.nodes <- selectMostImportant(evening.rush, series = c(series), 
                    rho=0.3, topn = 50, shortest=mysample$shortest.distances,distance.lim=1)
length(evening.nodes)
write.csv(mysample$config.nodes%>%filter(node_name %in% evening.nodes),file.path(dir,"nodes-ER50.csv"),  row.names = F)

write.csv(mysample$config.nodes%>%filter(node_name %in% gsub('.volume','',intersect(morning.nodes,evening.nodes))),file.path(dir,"nodes-ER+MR.csv"),  row.names = F)
intersect(morning.nodes,evening.nodes)
length(union(morning.nodes,evening.nodes))
series <- union(morning.nodes,evening.nodes)

dat<-mysample$data.orig%>%gather(key = "node", value="value", -datetime)%>%filter(node %in% paste0(series,".volume"))%>%mutate(node=gsub(".volume","",node), dt=as.POSIXct(format(datetime, "%H:%M"),format="%H:%M",tz="GMT"))%>%group_by(node,dt)%>%summarize(value=median(value))
date.labels = seq(from = min(dat$dt),
                 to = max(dat$dt),
                 by = "hour")
dat%>%filter(node %in% morning.nodes)%>%ggplot(aes(x = dt, y = value, col=node, group=node)) + geom_line()+ 
  theme(legend.position = "none")+scale_x_datetime(labels = date_format("%H:%M"),
                     date_breaks = "4 hours")+ xlab("TIme of the day") + ylab("Traffic volume")
dat%>%filter(node %in% evening.nodes)%>%ggplot(aes(x = dt, y = value, col=node, group=node))  + geom_line()+ 
  theme(legend.position = "none")+scale_x_datetime(labels = date_format("%H:%M"),
                     date_breaks = "4 hours")+ xlab("TIme of the day") + ylab("Traffic volume")

sampl<-list(data = mysample$data[,c("datetime",series)],
               shortest.distances=mysample$shortest.distances[series,series])
saveRDS(sampl, file=sample.rds)
}else{
  sampl <- readRDS(sample.rds)
}
print(ncol(sampl$data))
series <- colnames(sampl$data)[-1]

# Prepare a matrix of maximal lags (for travel time regularisation)
lagMatrix <- round(sampl$shortest.distances / ta)
lagMatrix[is.infinite(lagMatrix)]<-0
lagMatrix<-lagMatrix[as.vector(series),as.vector(series)]


shortestA <- sampl$shortest.distances

d<-c()
for (i in seq(1, 60)){
  d<-append(d,length(shortestA[shortestA<i])/ncol(shortestA))
}
dat<-tibble(x=seq(1, 60), d=d)
ggplot(dat, aes(x, d)) + geom_line(colour="red")+xlab("Travel time")+ylab("% of reached nodes")

d<-tibble(av=as.vector(shortestA[shortestA<Inf]))
ggplot(d, aes(av)) + geom_density(alpha = 0.1,fill="red", colour="red")

gr<-shortestA
gr[gr>15]<-0
mean(gr[gr>0])
needs(igraph)
g<-graph_from_adjacency_matrix(gr, mode = "directed", weighted = T)
#tkplot(g, vertex.size=7, vertex.color="lightblue", vertex.label=NA,edge.arrow.size=0.5, edge.width=E(g)$weight*0.1)
comp<-components(g, mode = "weak")
```


```{r fs}
install.packages("fsMTS", repos="http://R-Forge.R-project.org")
needs(fsMTS)
needs(plot.matrix)
source(file.path("R","cv_functions.R"))
data.scaled<-cbind(sampl$data[,1],scale(sampl$data[,-1]))
estimateFeatures(fs.function=fsMTS::fsMTS, 
                 data = sampl$data, 
                 seriesNames = series,
                 trainingWindowSize = 36,#12*24*0.5,
                 forecastEvery = 1,
                 max.lag = 3,
                 fs.folder = file.path(dir,"fs",length(series),"CCF"),
                 req.packages = c("fsMTS"),
                 method="CCF",
                 rho=0.1,
                 shortest = shortestA,
                 step = ta,
                 localized = F
                 )
```

```{r models}
RunModels <- function(data, series, ta, forecastingSteps, forecastEvery,
                      validationSize, validationEnd, 
                      lagMatrix,shortestA, cvgrid, 
                      models.estimated.file, results.file,save.links.file){
  start.time <- Sys.time()
  results<-tibble()
  models.estimated <- c()
  if (file.exists(models.estimated.file)) models.estimated <- readRDS(models.estimated.file)
  else saveRDS(models.estimated, models.estimated.file)
  if (file.exists(results.file)) results <- readRDS(results.file)
  else saveRDS(results, results.file)
  for (r in 1:nrow(cvgrid)){
    cv <- cvgrid[r,]
    print(paste("Tuned parameter's set",r,"of",nrow(cvgrid)))
    print(cv)
    validationStart <- validationEnd - validationSize
    trainingSize <- cv$trainingMinutes/ta
    dat.restricted<-data[(validationStart-trainingSize+1):validationEnd,]
    base_params<-list(data=dat.restricted, seriesNames=series,
                      forecastingSteps=forecastingSteps, forecastEvery=forecastEvery)
    params<-c(base_params,list(trainingWindowSize=trainingSize))
    
    if (length(forecastingSteps)>1) params$forecastingSteps=max(forecastingSteps)
    estimate.HA(params,cv,results.file, models.estimated.file)
    estimate.arima(params,cv,results.file, models.estimated.file)
    estimate.SpVAR(params,cv,results.file, models.estimated.file)
    #estimate.SpVARcc(params,cv,results.file, models.estimated.file)
    print(Sys.time() - start.time)
  }
}
```

```{r experiments}
dat<-as.data.frame(sampl$data)
rownames(dat)<-format(dat$datetime,"%Y-%m-%d_%H%M%S")

fs.dir<-file.path(dir,"fs",length(series))
xModel.star$run(dat[1:576,series], forecastingSteps=3, arLags=3, 
                fs.folder=file.path(fs.dir,"CCF",144,3), 
                control=list(),
                returnModel=F, 
                refine = F, 
                include.mean=F, 
                verbose=T,
                exclude.series=NULL,
                threshold=0.01)

validationSize <- nrow(sampl$data)-12*60*10/ta
validationEnd <- nrow(sampl$data)
forecastEvery<-3*24
forecastingSteps<-12

# Define a grid for tuning parameters
cvgrid <- expand.grid(trainingMinutes = c(12*60*10),
                      include.mean = c(F),
                      arLags = c(3),
                      stationary =  c(F),
                      allowdrift =  c(F),
                      fs.folder=c(NA,
                                  file.path(fs.dir,"distance",288,3),
                                  file.path(fs.dir,"CCF",288,3),
                                  file.path(fs.dir,"CCFscaled",288,3)),
                      threshold=c(0.1),
                      ownlags=c(F))
source(file.path("R","models_spvar.R"))

# Run all models
RunModels(sampl$data, series, ta, forecastingSteps, forecastEvery,
                      validationSize, validationEnd, 
                      lagMatrix, shortestA, cvgrid, 
                      models.estimated.file, results.file,save.links.file)
 
```

```{r analyse_results}
params <- c('trainingMinutes','allowdrift','stationary','arLags','include.mean','fs.folder','threshold','ownlags')

results <- readRDS(results.file)
results<-results%>%mutate(fs.folder=gsub(file.path(dir,"fs"),"",fs.folder))
models.estimated <- readRDS(models.estimated.file)
summary(results$actual)
#results<-results%>%filter(abs(actual)>100)
results%>%select(datetime,detector,actual,forecasted)
actu<-mysample$data.orig%>%gather(key="detector", value="actual.orig", -one_of("datetime"))%>%
  mutate(detector=gsub(".volume","",detector))
r<-results%>%left_join(actu, by=c("datetime"="datetime","detector"="detector"))
r1<-r%>%mutate(actual=actual+actual.orig,forecasted=forecasted+actual.orig)
mymape <- function(actual,pred){
          d<-abs((actual - pred)/actual)
          d[d==Inf]<-NA
           mape <- mean(d, na.rm = T)*100
           return (mape)
         }

s<-cvSummary(r1, mymape, cumulative=F, params=params)%>%
    mutate(modelid=paste(model,trainingMinutes, arLags, include.mean,fs.folder,threshold,ownlags))%>%
    mutate(modelid=gsub(" NA","",modelid))
#saveRDS(s, summary.file)

options(pillar.sigfig = 4)
s%>%
    filter(forecast_horizon==3)%>%
    group_by(modelid)%>%filter(MYMAPE==min(MYMAPE))%>%ungroup%>%select(-c('allowdrift','stationary','include.mean'))%>%print(n=50)


(winners3 <- s%>%
    filter(forecast_horizon==3)%>%
    group_by(model)%>%filter(MAE==min(MAE))%>%ungroup%>%select(-c('allowdrift','stationary','include.mean')))


 results.cum <-results%>%
    mutate(modelid=gsub(" NA","",paste(model,trainingMinutes, arLags, ccfThreshold,
                         include.mean,stationary,allowdrift,radius1,radius2,include_ratio, spatial_step,modelsp,decay)))%>%
    group_by(model_hash,detector)%>%
    arrange(model_hash,detector,forecast_horizon)%>%
    mutate(cumactual=cumsum(actual),cumforecasted=cumsum(forecasted))

#tmodels<-c("autoarima 7200 FALSE FALSE FALSE","SpatialARIMAX 7200 FALSE FALSE FALSE 10 30 FALSE 5",           "SpatialSVR 7200 10 30 FALSE 5 svm","STAR ccf 7200 6 0.1 FALSE")
 tmodels<-c("autoarima 7200 FALSE FALSE FALSE","SpatialARIMAX 7200 FALSE FALSE FALSE 10 30 FALSE 5",           "SpatialSVR 7200 10 30 FALSE 5 svm")
results.cum.filtered<-results.cum%>%filter(forecast_horizon==3,modelid %in% tmodels)%>% mutate(modelid=ifelse(str_detect(modelid,"^STAR ccf"),"SpVAR-cc",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^SpatialARIMAX"),"SpX-ARIMAX",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^SpatialSVR"),"SpX-SVR",modelid))%>% mutate(modelid=ifelse(str_detect(modelid,"^autoarima"),"ARIMA",modelid))
 
gtmp1<-results.cum.filtered%>%mutate(last_date_d = format(datetime, format="%H:%M",tz="GMT"))

gtmp2<-gtmp1%>%group_by(modelid,last_date_d)%>%summarise(mn=mae(cumactual,cumforecasted))
gtmp2%>%ggplot(aes(x = mn, group=modelid, fill=modelid)) +
  geom_density(kernel = "epanechnikov", alpha = 0.4)+
  scale_x_continuous(expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+labs(x="Temporarily averaged MAE", fill="Model")
 
gtmp2<-gtmp1%>%group_by(modelid,detector)%>%summarise(mn=mae(cumactual,cumforecasted))
gtmp2%>%ggplot(aes(x = mn, group=modelid, fill=modelid)) +
  geom_density(kernel = "epanechnikov", alpha = 0.4)+
  scale_x_continuous(expand=c(0,0))+
  scale_y_continuous(expand=c(0,0))+labs(x="Spatially averaged MAE", fill="Model")
 
```